{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import unicodedata\n",
    "import re\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.functional as F\n",
    "import numpy as np\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "SOS_TOKEN = 0\n",
    "EOS_TOKEN = 1\n",
    "\n",
    "class Lang:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2index = {}\n",
    "        # used later to replace rare words\n",
    "        self.word2count = {}\n",
    "        self.index2word = {0: \"SOS\", 1: \"EOS\"}\n",
    "        self.n_words = 2\n",
    "    \n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.word2count[word] = 1\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1\n",
    "    \n",
    "    def addSentence(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.addWord(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn unicode to ascii\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        # break word down into its base plus the accent if applicable\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        # only return the chars which are valid roman letters\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "\n",
    "def formatString(s: str):\n",
    "    s = unicodeToAscii(s.lower().strip())\n",
    "    # add space before punctuation to treat it like its own token\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    # replace any non-tokenized characters with a space so they do not affect the data\n",
    "    s = re.sub(r\"[^a-zA-Z!?]+\", r\" \", s)\n",
    "    return s.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I usually have trouble speaking the language, but I can understand just fine. For this implementation, I'm going to supplement my learning and see if the model can translate from english to spanish (my weakness) better than I can"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readLines(spa_to_eng=False):\n",
    "    print(\"reading lines\")\n",
    "    \n",
    "    # split each pair into its own element\n",
    "    lines = open(\"data/spa-eng/cleaned.txt\", encoding='utf-8').read().strip().split('\\n')\n",
    "    # format the strings and store the english to spanish pairs together\n",
    "    pairs = [[formatString(s) for s in line.split('\\t')] for line in lines]\n",
    "    \n",
    "    if spa_to_eng:\n",
    "        pairs = [list(reversed(p)) for p in pairs]\n",
    "        input_lang = Lang(\"spa\")\n",
    "        output_lang = Lang(\"end\")\n",
    "    else:\n",
    "        input_lang = Lang(\"eng\")\n",
    "        output_lang = Lang(\"spa\")\n",
    "    \n",
    "    return input_lang, output_lang, pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Limiting training data for initial passes and to make sure approach works. Will slowly incorporate more data later as I find better/faster ways to train this large model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_prefixes = (\n",
    "    \"i am \", \"i m \",\n",
    "    \"he is\", \"he s \",\n",
    "    \"she is\", \"she s \",\n",
    "    \"you are\", \"you re \",\n",
    "    \"we are\", \"we re \",\n",
    "    \"they are\", \"they re \"\n",
    ")\n",
    "\n",
    "def isValidPair(pair):\n",
    "    source, target = pair\n",
    "    return source.startswith(eng_prefixes) or target.startswith(eng_prefixes)\n",
    "\n",
    "def filterPairs(pairs):\n",
    "    return [pair for pair in pairs if isValidPair(pair)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading lines\n",
      "Read 142511 sentence pairs\n",
      "Trimmed to 10584 sentence pairs\n",
      "Counting words... \n",
      "\n",
      "Counted words:\n",
      "eng 3341\n",
      "spa 5006\n",
      "['she s been learning german for a year now', 'ella ha estado aprendiendo aleman por un ano']\n",
      "['he isn t happy at all', 'el no esta contento en absoluto']\n",
      "['i m a university student', 'soy universitario']\n",
      "['he s known for that', 'el es conocido por eso']\n",
      "['he is younger than me by three years', 'tiene tres anos menos que yo']\n",
      "['i m from australia', 'soy de australia']\n",
      "['i m breaking up with my girlfriend tonight', 'esta noche terminare con mi novia']\n",
      "['i m going to the other side of the world', 'me voy al otro lado del mundo']\n",
      "['i m boiling water', 'estoy hirviendo agua']\n",
      "['she is wearing a nice dress now', 'ahora lleva puesto un precioso vestido']\n"
     ]
    }
   ],
   "source": [
    "def prepareData(spa_to_eng=False):\n",
    "    input_lang, output_lang, pairs = readLines(spa_to_eng)\n",
    "    print(\"Read %s sentence pairs\" % len(pairs))\n",
    "    pairs = filterPairs(pairs)\n",
    "    print(\"Trimmed to %s sentence pairs\" % len(pairs))\n",
    "    print(\"Counting words...\", '\\n')\n",
    "    for pair in pairs:\n",
    "        input_lang.addSentence(pair[0])\n",
    "        output_lang.addSentence(pair[1])\n",
    "    print(\"Counted words:\")\n",
    "    print(input_lang.name, input_lang.n_words)\n",
    "    print(output_lang.name, output_lang.n_words)\n",
    "    return input_lang, output_lang, pairs\n",
    "\n",
    "input_lang, output_lang, pairs = prepareData()\n",
    "for _ in range(5):\n",
    "    print(random.choice(pairs))\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
