{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import unicodedata\n",
    "import re\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this network, it is typical to encode each word as a one-hot vector. For this, I will utilize a Lang class that holds on to the index of each word as it appears. The model will then be trained to find similarities between words to encode them with meaning. This is a compute-heavy process and it may be better to use pre-trained vectors, so I may return and switch to something like word2vec once I complete my initial implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SOS_TOKEN = 0\n",
    "EOS_TOKEN = 1\n",
    "\n",
    "class Lang:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2index = {}\n",
    "        # used later to replace rare words\n",
    "        self.word2count = {}\n",
    "        self.index2word = {0: \"SOS\", 1: \"EOS\"}\n",
    "        self.n_words = 2\n",
    "    \n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.word2count[word] = 1\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1\n",
    "    \n",
    "    def addSentence(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.addWord(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn unicode to ascii\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        # break word down into its base plus the accent if applicable\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        # only return the chars which are valid roman letters\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "\n",
    "def formatString(s: str):\n",
    "    s = unicodeToAscii(s.lower().strip())\n",
    "    # add space before punctuation to treat it like its own token\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    # replace any non-tokenized characters with a space so they do not affect the data\n",
    "    s = re.sub(r\"[^a-zA-Z!?]+\", r\" \", s)\n",
    "    return s.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I usually have trouble speaking the language, but I can understand just fine. For this implementation, I'm going to supplement my learning and see if the model can translate from english to spanish (my weakness) better than I can. However, we will also use a flag to simply reverse the direction at any time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readLines(spa_to_eng=False):\n",
    "    print(\"Reading lines\")\n",
    "    \n",
    "    # split each pair into its own element\n",
    "    lines = open(\"data/spa-eng/cleaned.txt\", encoding='utf-8').read().strip().split('\\n')\n",
    "    # format the strings and store the english to spanish pairs together\n",
    "    pairs = [[formatString(s) for s in line.split('\\t')] for line in lines]\n",
    "    \n",
    "    # create language objects for use later\n",
    "    if spa_to_eng:\n",
    "        pairs = [list(reversed(p)) for p in pairs]\n",
    "        input_lang = Lang(\"spa\")\n",
    "        output_lang = Lang(\"end\")\n",
    "    else:\n",
    "        input_lang = Lang(\"eng\")\n",
    "        output_lang = Lang(\"spa\")\n",
    "    \n",
    "    return input_lang, output_lang, pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Limiting training data for initial passes and to make sure approach works. Will slowly incorporate more data later as I find better/faster ways to train this large model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_prefixes = (\n",
    "    \"i am \", \"i m \",\n",
    "    \"he is\", \"he s \",\n",
    "    \"she is\", \"she s \",\n",
    "    \"you are\", \"you re \",\n",
    "    \"we are\", \"we re \",\n",
    "    \"they are\", \"they re \"\n",
    ")\n",
    "\n",
    "# check to make sure a pair start with the above prefixes\n",
    "def isValidPair(pair):\n",
    "    source, target = pair\n",
    "    return source.startswith(eng_prefixes) or target.startswith(eng_prefixes)\n",
    "\n",
    "# apply validity to all pairs\n",
    "def filterPairs(pairs):\n",
    "    return [pair for pair in pairs if isValidPair(pair)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading lines\n",
      "Read 142511 sentence pairs\n",
      "Trimmed to 10584 sentence pairs\n",
      "Counting words... \n",
      "\n",
      "Counted words:\n",
      "eng 3341\n",
      "spa 5006\n",
      "['i m not the same fool i was fifteen years ago', 'yo no soy el mismo tonto que era hace quince anos']\n",
      "['you re too drunk', 'sos demasiado borracho']\n",
      "['we re looking for him', 'lo estamos buscando']\n",
      "['i m not so sure that was a good idea', 'no estoy tan seguro de que fuera buena idea']\n",
      "['they re not following me', 'no me estan siguiendo']\n"
     ]
    }
   ],
   "source": [
    "def prepareData(spa_to_eng=False):\n",
    "    # read in all liens\n",
    "    input_lang, output_lang, pairs = readLines(spa_to_eng)\n",
    "    print(\"Read %s sentence pairs\" % len(pairs))\n",
    "    \n",
    "    # filter down pairs for easier training\n",
    "    pairs = filterPairs(pairs)\n",
    "    print(\"Trimmed to %s sentence pairs\" % len(pairs))\n",
    "    \n",
    "    # populate the Language objects\n",
    "    print(\"Counting words...\", '\\n')\n",
    "    for pair in pairs:\n",
    "        input_lang.addSentence(pair[0])\n",
    "        output_lang.addSentence(pair[1])\n",
    "    print(\"Counted words:\")\n",
    "    print(input_lang.name, input_lang.n_words)\n",
    "    print(output_lang.name, output_lang.n_words)\n",
    "    return input_lang, output_lang, pairs\n",
    "\n",
    "input_lang, output_lang, pairs = prepareData()\n",
    "for _ in range(5):\n",
    "    print(random.choice(pairs))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, I am using a sequence to sequence model to simulate the many to many relationship necessary for translation. This uses two RNNs, one to encode the input words and one to decode this interpretation as translation output. In a single RNN, every input corresponds to an output, but in a seq2seq model we do not have to worry about the order of the words or the number of words in the sentence. This makes it great for translation because usually two languages will not use the same number of words in the same order for the same phrase. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder\n",
    "\n",
    "The encoder is a RNN that takes the input words and turns all the words into a single point in n dimensional space. Ideally, this vector holds the meaning of the sentence. \n",
    "\n",
    "As is typical with a RNN, the encoder outputs hidden state and output vectors, then uses that hidden state again for the next word prediction (hence the Recurrent in Recurrent Neural Networks)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    '''\n",
    "    Initialize the encoder:\n",
    "    input_size= vocabulary size\n",
    "    hidden_size= size of GRU hidden state and embedding\n",
    "    dropout_p= probability of dropout\n",
    "    '''\n",
    "    def __init__(self, input_size, hidden_size, dropout_p=0.1):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        # Convert each word token to a size hidden_size dense vector embedding.\n",
    "        # Shape: (batch_size, seq_length) -> (batch_size, seq_length, hidden_size)\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        # gated recurrent unit to process context\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, batch_first=True)\n",
    "        # set embeddings to 0 randomly to prevent overfitting\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "    \n",
    "    '''\n",
    "    Pass input through the encoder.\n",
    "    X: shape(batch_size, sequence_length) tensor of token indices\n",
    "    '''\n",
    "    def forward(self, x):\n",
    "        embedded = self.dropout(self.embedding(x))\n",
    "        # (batch_size, seq_length, hidden_size), (1, batch_size, hidden_size)\n",
    "        output, hidden = self.gru(embedded)\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention Decoder\n",
    "\n",
    "We are using attention to allow the decoder to focus on different parts of the encoder's outputs at each step instead of relying on the one output vector to carry meaning for the whole sentence. \n",
    "\n",
    "We are using Bahadanau attention because it's the one I know more about. Will come back and explore different types of attention later. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahadanauAttention(nn.Module):\n",
    "    # init attention model\n",
    "    def __init__(self, hidden_size):\n",
    "        super(BahadanauAttention, self).__init__()\n",
    "        # linear layer applied to query (decoder)\n",
    "        self.Wa = nn.Linear(hidden_size, hidden_size)\n",
    "        # linear layer applied to keys (encoder)\n",
    "        self.Ua = nn.Linear(hidden_size, hidden_size)\n",
    "        # linear layer that produces a scalar attention score\n",
    "        self.Va = nn.Linear(hidden_size, 1)\n",
    "        \n",
    "    def forward(self, query, keys):\n",
    "        # query: (batch_size, hidden_size)\n",
    "        # keys: (batch_size, seq_len, hidden_size)\n",
    "        scores = self.Va(torch.tanh(self.Wa(query) + self.Ua(keys)))\n",
    "        # (batch_size, seq_len, 1) -> (batch_size, seq_len) -> (batch_size, 1, seq_len)\n",
    "        scores = scores.squeeze(2).unsqueeze(1)\n",
    "        \n",
    "        # find attention weights accross encoder steps\n",
    "        weights = F.softmax(scores, dim=-1)\n",
    "        # batch matmul to find weighted sum of encoder hidden state attention scores\n",
    "        context = torch.bmm(weights, keys)\n",
    "        \n",
    "        return context, weights\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttnDecoder(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, dropout_p=0.1):\n",
    "        super(AttnDecoder, self).__init__()\n",
    "        # again takes vocab word and turns it into a hidden_size vector\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        # using Bahdanau attention\n",
    "        self.attention = BahadanauAttention(hidden_size)\n",
    "        self.gru = nn.GRU(2 * hidden_size, hidden_size, batch_first=True)\n",
    "        # simply maps gru to correct output size\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        # regularization\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "    \n",
    "    '''\n",
    "    Perform computation on one token (step)\n",
    "    In:\n",
    "        - input: current index of token (batch_size, 1)\n",
    "        - hidden: previous hidden state (1, batch_size, hidden_size)\n",
    "        - encoder_outputs: all encoder states (batch_size, seq_len, hidden_size)\n",
    "    Out:\n",
    "        - output: vocab logits for current step (batch_size, 1)\n",
    "        - hidden: new hidden state\n",
    "        - attn_weights: attention dist for the step (batch_size, 1, seq_len)\n",
    "    '''\n",
    "    def forward_step(self, input, hidden, encoder_outputs):\n",
    "        # (batch_size, 1) -> (batch_size, 1, hidden_size)\n",
    "        embedded = self.dropout(self.embedding(input))\n",
    "        \n",
    "        # (1, batch_size, hidden_size) -> (batch_size, 1, hidden_size)\n",
    "        # to match encoder outputs to attention\n",
    "        query = hidden.permute(1, 0, 2)\n",
    "        context, attn_weights = self.attention(query, encoder_outputs)\n",
    "        # (batch_size, 1, 2 * hidden_size)\n",
    "        input_gru = torch.cat((embedded, context), dim=2)\n",
    "        \n",
    "        # output: (batch_size, 1, hidden_size)\n",
    "        # hidden: (1, batch_size, hidden_size)\n",
    "        output, hidden = self.gru(input_gru, hidden)\n",
    "        # projects gru outputs to vocab\n",
    "        output = self.out(output)\n",
    "        \n",
    "        return output, hidden, attn_weights\n",
    "    \n",
    "    '''\n",
    "    Predicts the output sequence of tokens\n",
    "    In:\n",
    "        - encoder_outputs: (batch_size, seq_len, hidden_size)\n",
    "        - encoder_hidden: final encoder hidden state (1, batch_size, hidden_state)\n",
    "        - target_tensor: (batch_size, max_len) for teacher forcing\n",
    "    Out:\n",
    "        - decoder_outputs: encoding of each predicted token in the sequence\n",
    "        - decoder_hidden: final hidden state\n",
    "        - attentions: all attention weights for visualization\n",
    "    '''\n",
    "    def forward(self, encoder_outputs, encoder_hidden, target_tensor=None):\n",
    "        batch_size = encoder_outputs.size(0)\n",
    "        \n",
    "        # initialize with sos start for all token sequences\n",
    "        decoder_input = torch.empty(batch_size, 1, dtype=torch.long, device=device).fill_(SOS_TOKEN)\n",
    "        decoder_hidden = encoder_hidden\n",
    "        decoder_outputs = []\n",
    "        # (batch_size, max_decode_len, seq_len)\n",
    "        attentions = []\n",
    "        \n",
    "        for i in range(20):\n",
    "            # run a single time step of decoding with attention\n",
    "            decoder_output, decoder_hidden, attn_weights = self.forward_step(\n",
    "                decoder_input, decoder_hidden, encoder_outputs\n",
    "            )\n",
    "            # collect outputs and attentions\n",
    "            decoder_outputs.append(decoder_output)\n",
    "            attentions.append(attn_weights)\n",
    "            \n",
    "            # Teacher forcing\n",
    "            # if target tensor is provided, use ground truth otherwise, pick the best prediction as the next input\n",
    "            if target_tensor is not None:\n",
    "                # teacher forcing\n",
    "                decoder_input = target_tensor[:, i].unsqueeze(1)\n",
    "            else:\n",
    "                _, topi = decoder_output.topk(1)\n",
    "                decoder_input = topi.squeeze(-1).detach()\n",
    "        \n",
    "        \n",
    "        decoder_outputs = torch.cat(decoder_outputs, dim=1)\n",
    "        decoder_outputs = F.log_softmax(decoder_outputs, dim=-1)\n",
    "        attentions = torch.cat(attentions, dim=1)\n",
    "        \n",
    "        return decoder_outputs, decoder_hidden, attentions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
