{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import unicodedata\n",
    "import re\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this network, it is typical to encode each word as a one-hot vector. For this, I will utilize a Lang class that holds on to the index of each word as it appears. The model will then be trained to find similarities between words to encode them with meaning. This is a compute-heavy process and it may be better to use pre-trained vectors, so I may return and switch to something like word2vec once I complete my initial implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SOS_TOKEN = 0\n",
    "EOS_TOKEN = 1\n",
    "\n",
    "class Lang:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2index = {}\n",
    "        # used later to replace rare words\n",
    "        self.word2count = {}\n",
    "        self.index2word = {0: \"SOS\", 1: \"EOS\"}\n",
    "        self.n_words = 2\n",
    "    \n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.word2count[word] = 1\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1\n",
    "    \n",
    "    def addSentence(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.addWord(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn unicode to ascii\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        # break word down into its base plus the accent if applicable\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        # only return the chars which are valid roman letters\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "\n",
    "def formatString(s: str):\n",
    "    s = unicodeToAscii(s.lower().strip())\n",
    "    # add space before punctuation to treat it like its own token\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    # replace any non-tokenized characters with a space so they do not affect the data\n",
    "    s = re.sub(r\"[^a-zA-Z!?]+\", r\" \", s)\n",
    "    return s.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I usually have trouble speaking the language, but I can understand just fine. For this implementation, I'm going to supplement my learning and see if the model can translate from english to spanish (my weakness) better than I can. However, we will also use a flag to simply reverse the direction at any time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readLines(spa_to_eng=False):\n",
    "    print(\"Reading lines\")\n",
    "    \n",
    "    # split each pair into its own element\n",
    "    lines = open(\"data/spa-eng/cleaned.txt\", encoding='utf-8').read().strip().split('\\n')\n",
    "    # format the strings and store the english to spanish pairs together\n",
    "    pairs = [[formatString(s) for s in line.split('\\t')] for line in lines]\n",
    "    \n",
    "    # create language objects for use later\n",
    "    if spa_to_eng:\n",
    "        pairs = [list(reversed(p)) for p in pairs]\n",
    "        input_lang = Lang(\"spa\")\n",
    "        output_lang = Lang(\"end\")\n",
    "    else:\n",
    "        input_lang = Lang(\"eng\")\n",
    "        output_lang = Lang(\"spa\")\n",
    "    \n",
    "    return input_lang, output_lang, pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Limiting training data for initial passes and to make sure approach works. Will slowly incorporate more data later as I find better/faster ways to train this large model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 15\n",
    "\n",
    "eng_prefixes = (\n",
    "    \"i am \", \"i m \",\n",
    "    \"he is\", \"he s \",\n",
    "    \"she is\", \"she s \",\n",
    "    \"you are\", \"you re \",\n",
    "    \"we are\", \"we re \",\n",
    "    \"they are\", \"they re \"\n",
    ")\n",
    "\n",
    "# check to make sure a pair start with the above prefixes\n",
    "def isValidPair(pair):\n",
    "    source, target = pair\n",
    "    return (len(source.split(' ')) < MAX_LENGTH\n",
    "            and len(target.split(' ')) < MAX_LENGTH\n",
    "            and source.startswith(eng_prefixes) or target.startswith(eng_prefixes))\n",
    "\n",
    "# apply validity to all pairs\n",
    "def filterPairs(pairs):\n",
    "    return [pair for pair in pairs if isValidPair(pair)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading lines\n",
      "Read 142511 sentence pairs\n",
      "Trimmed to 10491 sentence pairs\n",
      "Counting words... \n",
      "\n",
      "Counted words:\n",
      "eng 3299\n",
      "spa 4933\n",
      "['i m sorry tom i m afraid i can t do that', 'lo siento tom me temo que no puedo hacer eso']\n",
      "['you re not doing your share', 'no estas cumpliendo con tu parte']\n",
      "['i m snowed under with work', 'estoy hasta el cuello con el trabajo']\n",
      "['i m not going to let anybody stop me', 'no voy a permitir que nadie me detenga']\n",
      "['i m clean', 'soy limpio']\n"
     ]
    }
   ],
   "source": [
    "def prepareData(spa_to_eng=False):\n",
    "    # read in all liens\n",
    "    input_lang, output_lang, pairs = readLines(spa_to_eng)\n",
    "    print(\"Read %s sentence pairs\" % len(pairs))\n",
    "    \n",
    "    # filter down pairs for easier training\n",
    "    pairs = filterPairs(pairs)\n",
    "    print(\"Trimmed to %s sentence pairs\" % len(pairs))\n",
    "    \n",
    "    # populate the Language objects\n",
    "    print(\"Counting words...\", '\\n')\n",
    "    for pair in pairs:\n",
    "        input_lang.addSentence(pair[0])\n",
    "        output_lang.addSentence(pair[1])\n",
    "    print(\"Counted words:\")\n",
    "    print(input_lang.name, input_lang.n_words)\n",
    "    print(output_lang.name, output_lang.n_words)\n",
    "    return input_lang, output_lang, pairs\n",
    "\n",
    "input_lang, output_lang, pairs = prepareData()\n",
    "for _ in range(5):\n",
    "    print(random.choice(pairs))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, I am using a sequence to sequence model to simulate the many to many relationship necessary for translation. This uses two RNNs, one to encode the input words and one to decode this interpretation as translation output. In a single RNN, every input corresponds to an output, but in a seq2seq model we do not have to worry about the order of the words or the number of words in the sentence. This makes it great for translation because usually two languages will not use the same number of words in the same order for the same phrase. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder\n",
    "\n",
    "The encoder is a RNN that takes the input words and turns all the words into a single point in n dimensional space. Ideally, this vector holds the meaning of the sentence. \n",
    "\n",
    "As is typical with a RNN, the encoder outputs hidden state and output vectors, then uses that hidden state again for the next word prediction (hence the Recurrent in Recurrent Neural Networks)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    '''\n",
    "    Initialize the encoder:\n",
    "    input_size= vocabulary size\n",
    "    hidden_size= size of GRU hidden state and embedding\n",
    "    dropout_p= probability of dropout\n",
    "    '''\n",
    "    def __init__(self, input_size, hidden_size, dropout_p=0.1):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        # Convert each word token to a size hidden_size dense vector embedding.\n",
    "        # Shape: (batch_size, seq_length) -> (batch_size, seq_length, hidden_size)\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        # gated recurrent unit to process context\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, batch_first=True)\n",
    "        # set embeddings to 0 randomly to prevent overfitting\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "    \n",
    "    '''\n",
    "    Pass input through the encoder.\n",
    "    X: shape(batch_size, sequence_length) tensor of token indices\n",
    "    '''\n",
    "    def forward(self, x):\n",
    "        embedded = self.dropout(self.embedding(x))\n",
    "        # (batch_size, seq_length, hidden_size), (1, batch_size, hidden_size)\n",
    "        output, hidden = self.gru(embedded)\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention Decoder\n",
    "\n",
    "We are using attention to allow the decoder to focus on different parts of the encoder's outputs at each step instead of relying on the one output vector to carry meaning for the whole sentence. \n",
    "\n",
    "We are using Bahadanau attention because it's the one I know more about. Will come back and explore different types of attention later. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahadanauAttention(nn.Module):\n",
    "    # init attention model\n",
    "    def __init__(self, hidden_size):\n",
    "        super(BahadanauAttention, self).__init__()\n",
    "        # linear layer applied to query (decoder)\n",
    "        self.Wa = nn.Linear(hidden_size, hidden_size)\n",
    "        # linear layer applied to keys (encoder)\n",
    "        self.Ua = nn.Linear(hidden_size, hidden_size)\n",
    "        # linear layer that produces a scalar attention score\n",
    "        self.Va = nn.Linear(hidden_size, 1)\n",
    "        \n",
    "    def forward(self, query, keys):\n",
    "        # query: (batch_size, hidden_size)\n",
    "        # keys: (batch_size, seq_len, hidden_size)\n",
    "        scores = self.Va(torch.tanh(self.Wa(query) + self.Ua(keys)))\n",
    "        # (batch_size, seq_len, 1) -> (batch_size, seq_len) -> (batch_size, 1, seq_len)\n",
    "        scores = scores.squeeze(2).unsqueeze(1)\n",
    "        \n",
    "        # find attention weights accross encoder steps\n",
    "        weights = F.softmax(scores, dim=-1)\n",
    "        # batch matmul to find weighted sum of encoder hidden state attention scores\n",
    "        context = torch.bmm(weights, keys)\n",
    "        \n",
    "        return context, weights\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttnDecoder(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, dropout_p=0.1):\n",
    "        super(AttnDecoder, self).__init__()\n",
    "        # again takes vocab word and turns it into a hidden_size vector\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        # using Bahdanau attention\n",
    "        self.attention = BahadanauAttention(hidden_size)\n",
    "        self.gru = nn.GRU(2 * hidden_size, hidden_size, batch_first=True)\n",
    "        # simply maps gru to correct output size\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        # regularization\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "    \n",
    "    '''\n",
    "    Perform computation on one token (step)\n",
    "    In:\n",
    "        - input: current index of token (batch_size, 1)\n",
    "        - hidden: previous hidden state (1, batch_size, hidden_size)\n",
    "        - encoder_outputs: all encoder states (batch_size, seq_len, hidden_size)\n",
    "    Out:\n",
    "        - output: vocab logits for current step (batch_size, 1)\n",
    "        - hidden: new hidden state\n",
    "        - attn_weights: attention dist for the step (batch_size, 1, seq_len)\n",
    "    '''\n",
    "    def forward_step(self, input, hidden, encoder_outputs):\n",
    "        # (batch_size, 1) -> (batch_size, 1, hidden_size)\n",
    "        embedded = self.dropout(self.embedding(input))\n",
    "        \n",
    "        # (1, batch_size, hidden_size) -> (batch_size, 1, hidden_size)\n",
    "        # to match encoder outputs to attention\n",
    "        query = hidden.permute(1, 0, 2)\n",
    "        context, attn_weights = self.attention(query, encoder_outputs)\n",
    "        # (batch_size, 1, 2 * hidden_size)\n",
    "        input_gru = torch.cat((embedded, context), dim=2)\n",
    "        \n",
    "        # output: (batch_size, 1, hidden_size)\n",
    "        # hidden: (1, batch_size, hidden_size)\n",
    "        output, hidden = self.gru(input_gru, hidden)\n",
    "        # projects gru outputs to vocab\n",
    "        output = self.out(output)\n",
    "        \n",
    "        return output, hidden, attn_weights\n",
    "    \n",
    "    '''\n",
    "    Predicts the output sequence of tokens\n",
    "    In:\n",
    "        - encoder_outputs: (batch_size, seq_len, hidden_size)\n",
    "        - encoder_hidden: final encoder hidden state (1, batch_size, hidden_state)\n",
    "        - target_tensor: (batch_size, max_len) for teacher forcing\n",
    "    Out:\n",
    "        - decoder_outputs: encoding of each predicted token in the sequence\n",
    "        - decoder_hidden: final hidden state\n",
    "        - attentions: all attention weights for visualization\n",
    "    '''\n",
    "    def forward(self, encoder_outputs, encoder_hidden, target_tensor=None):\n",
    "        batch_size = encoder_outputs.size(0)\n",
    "        \n",
    "        # initialize with sos start for all token sequences\n",
    "        decoder_input = torch.empty(batch_size, 1, dtype=torch.long, device=device).fill_(SOS_TOKEN)\n",
    "        decoder_hidden = encoder_hidden\n",
    "        decoder_outputs = []\n",
    "        # (batch_size, max_decode_len, seq_len)\n",
    "        attentions = []\n",
    "        \n",
    "        for i in range(15):\n",
    "            # run a single time step of decoding with attention\n",
    "            decoder_output, decoder_hidden, attn_weights = self.forward_step(\n",
    "                decoder_input, decoder_hidden, encoder_outputs\n",
    "            )\n",
    "            # collect outputs and attentions\n",
    "            decoder_outputs.append(decoder_output)\n",
    "            attentions.append(attn_weights)\n",
    "            \n",
    "            # Teacher forcing\n",
    "            # if target tensor is provided, use ground truth otherwise, pick the best prediction as the next input\n",
    "            if target_tensor is not None:\n",
    "                # teacher forcing\n",
    "                decoder_input = target_tensor[:, i].unsqueeze(1)\n",
    "            else:\n",
    "                _, topi = decoder_output.topk(1)\n",
    "                decoder_input = topi.squeeze(-1).detach()\n",
    "        \n",
    "        \n",
    "        decoder_outputs = torch.cat(decoder_outputs, dim=1)\n",
    "        decoder_outputs = F.log_softmax(decoder_outputs, dim=-1)\n",
    "        attentions = torch.cat(attentions, dim=1)\n",
    "        \n",
    "        return decoder_outputs, decoder_hidden, attentions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Separating the outputs into tensor pairs (input and target) of tensors with each word as a different index in the tensor. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def indexesFromSentence(lang: Lang, sentence: str):\n",
    "    return [lang.word2index[word] for word in sentence.split(' ')]\n",
    "\n",
    "# take the index arr and add a dim for batching (batch size 1) = (seqlen,) -> (1, seqLen)\n",
    "def tensorFromSentence(lang: Lang, sentence: str):\n",
    "    indexes = indexesFromSentence(lang, sentence)\n",
    "    indexes.append(EOS_TOKEN)\n",
    "    return torch.tensor(indexes, dtype=torch.long, device=device).unsqueeze(0)\n",
    "\n",
    "# breaks input and target pair into two tensors\n",
    "def tensorFromPair(pair):\n",
    "    input_tensor = tensorFromPair(input_lang, pair[0])\n",
    "    target_tensor = tensorFromPair(output_lang, pair[1])\n",
    "    return input_tensor, target_tensor\n",
    "\n",
    "def getDataLoader(batch_size: int):\n",
    "    # get language objects and all data pairs\n",
    "    input_lang, output_lang, pairs = prepareData()\n",
    "    \n",
    "    # create two np arrays, one for input and one for target of the indexes for each sentence\n",
    "    n = len(pairs)\n",
    "    input_ids = np.zeros((n, MAX_LENGTH), dtype=np.int32)\n",
    "    target_ids = np.zeros((n, MAX_LENGTH), dtype=np.int32)\n",
    "    \n",
    "    for i, (inp, tgt) in enumerate(pairs):\n",
    "        # get indexes from each sentence\n",
    "        inp_ids = indexesFromSentence(input_lang, inp)\n",
    "        tgt_ids = indexesFromSentence(output_lang, tgt)\n",
    "        \n",
    "        # append eos to training data\n",
    "        inp_ids.append(EOS_TOKEN)\n",
    "        tgt_ids.append(EOS_TOKEN)\n",
    "        \n",
    "        # add the new indexed sentence to the collection of inputs and targets\n",
    "        input_ids[i, :len(inp_ids)] = inp_ids\n",
    "        target_ids[i, :len(tgt_ids)] = tgt_ids\n",
    "        \n",
    "    # create torch dataset utils to process the remaining data\n",
    "    train_data = TensorDataset(torch.LongTensor(input_ids).to(device), torch.LongTensor(target_ids).to(device))\n",
    "    train_sampler = RandomSampler(train_data)\n",
    "    train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "    return input_lang, output_lang, train_dataloader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(dataloader: DataLoader, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion):\n",
    "    total_loss = 0\n",
    "    for data in dataloader:\n",
    "        input_tensor, target_tensor = data\n",
    "        \n",
    "        # zero gradients for each pass\n",
    "        encoder_optimizer.zero_grad()\n",
    "        decoder_optimizer.zero_grad()\n",
    "        \n",
    "        encoder_outputs, encoder_hidden = encoder(input_tensor)\n",
    "        # teacher forcing on training\n",
    "        decoder_outputs, _, _ = decoder(encoder_outputs, encoder_hidden, target_tensor)\n",
    "        \n",
    "        # compute loss and perfrom backprop        \n",
    "        loss = criterion (\n",
    "            decoder_outputs.view(-1, decoder_outputs.size(-1)),\n",
    "            target_tensor.view(-1)\n",
    "        )\n",
    "        loss.backward()\n",
    "        encoder_optimizer.step()\n",
    "        decoder_optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper functions for tracking and analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "\n",
    "# convert a seconds timestamp to minutes + seconds\n",
    "def asMinute(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return \"%dm %ds\" % (m, s)\n",
    "\n",
    "# find elapsed seconds along with percentage of epochs complete for remaining seconds\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / percent\n",
    "    rs = es - s\n",
    "    return \"%s (-%s)\" % (asMinute(s), asMinute(rs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.switch_backend('agg')\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "def showPlot(points):\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots()\n",
    "    loc = ticker.MultipleLocator(base=0.2)\n",
    "    ax.yaxis.set_major_locator(loc)\n",
    "    plt.plot(points)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, train the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_dataloader, encoder: nn.Module, decoder: nn.Module, num_epochs, learning_rate=0.001, print_every=100, plot_every=100):\n",
    "    # initialize tracking vars\n",
    "    start = time.time()\n",
    "    plot_losses = []\n",
    "    print_loss_total = 0\n",
    "    plot_loss_total = 0\n",
    "    \n",
    "    # init training vars\n",
    "    encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
    "    decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate)\n",
    "    criterion = nn.NLLLoss()\n",
    "    \n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        # perform one pass on all data pairs and compute loss\n",
    "        loss = train_epoch(train_dataloader, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "        print_loss_total += loss\n",
    "        plot_loss_total += loss\n",
    "        \n",
    "        # print for tracking and reset loss agg for next pass\n",
    "        if epoch % print_every == 0:\n",
    "            print_avg = print_loss_total / print_every\n",
    "            print_loss_total = 0\n",
    "            print(\"%s (%d %d%%) %.4f\" % (timeSince(start, epoch / num_epochs),\n",
    "                                         epoch, epoch / num_epochs * 100, print_avg))\n",
    "        \n",
    "        # plot for tracking and reset loss agg for next pass\n",
    "        if epoch % plot_every == 0:\n",
    "            plot_avg = plot_loss_total / print_every\n",
    "            plot_losses.append(plot_avg)\n",
    "            plot_loss_total = 0\n",
    "    \n",
    "    showPlot(plot_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(encoder, decoder, sentence, input_lang: Lang, output_lang: Lang):\n",
    "    with torch.no_grad():\n",
    "        # create indexed tensor for outputs\n",
    "        input_tensor = tensorFromSentence(input_lang, sentence)\n",
    "        \n",
    "        # perform computations using encoding and passing into decoder\n",
    "        encoder_outputs, encoder_hidden = encoder(input_tensor)\n",
    "        decoder_out, decoder_hidden, decoder_attn = decoder(encoder_outputs, encoder_hidden)\n",
    "        \n",
    "        # grab output with the highest probability\n",
    "        # decoded_ids = decoder_out.argmax(dim=-1)\n",
    "        _, topi = decoder_out.topk(1)\n",
    "        decoded_ids = topi.squeeze()\n",
    "        \n",
    "        # map each output to a word\n",
    "        decoded_words = []\n",
    "        for idx in decoded_ids:\n",
    "            if idx.item() == EOS_TOKEN:\n",
    "                decoded_words.append('<EOS>')\n",
    "                break\n",
    "            decoded_words.append(output_lang.index2word[idx.item()])\n",
    "        \n",
    "        return decoded_words, decoder_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluateRandomly(encoder, decoder, n=10):\n",
    "    for i in range(n):\n",
    "        pair = random.choice(pairs)\n",
    "        print('>', pair[0])\n",
    "        print('=', pair[1])\n",
    "        output_words, _ = evaluate(encoder, decoder, pair[0], input_lang, output_lang)\n",
    "        output_sentence = ' '.join(output_words)\n",
    "        print('<', output_sentence)\n",
    "        print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading lines\n",
      "Read 142511 sentence pairs\n",
      "Trimmed to 10491 sentence pairs\n",
      "Counting words... \n",
      "\n",
      "Counted words:\n",
      "eng 3299\n",
      "spa 4933\n",
      "1m 26s (-21m 32s) (5 6%) 1.5721\n",
      "2m 48s (-19m 41s) (10 12%) 0.7835\n",
      "4m 13s (-18m 16s) (15 18%) 0.4614\n",
      "5m 38s (-16m 54s) (20 25%) 0.2936\n",
      "7m 1s (-15m 27s) (25 31%) 0.2059\n",
      "8m 24s (-14m 1s) (30 37%) 0.1559\n",
      "55m 14s (-71m 1s) (35 43%) 0.1260\n",
      "158m 37s (-158m 37s) (40 50%) 0.1062\n",
      "193m 25s (-150m 26s) (45 56%) 0.0927\n",
      "258m 41s (-155m 12s) (50 62%) 0.0838\n",
      "281m 41s (-128m 2s) (55 68%) 0.0771\n",
      "327m 16s (-109m 5s) (60 75%) 0.0723\n",
      "346m 9s (-79m 52s) (65 81%) 0.0681\n",
      "360m 9s (-51m 27s) (70 87%) 0.0651\n",
      "374m 5s (-24m 56s) (75 93%) 0.0627\n",
      "382m 7s (-0m 0s) (80 100%) 0.0606\n"
     ]
    }
   ],
   "source": [
    "hidden_size = 128\n",
    "batch_size = 32\n",
    "\n",
    "input_lang, output_lang, train_dataloader = getDataLoader(batch_size)\n",
    "\n",
    "encoder = Encoder(input_lang.n_words, hidden_size).to(device)\n",
    "decoder = AttnDecoder(hidden_size, output_lang.n_words).to(device)\n",
    "\n",
    "train(train_dataloader, encoder, decoder, 80, print_every=5, plot_every=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> he is a teacher and novelist\n",
      "= el es profesor y novelista\n",
      "< el es profesor y novelista <EOS>\n",
      "\n",
      "> she s not a child\n",
      "= ella no es una nina\n",
      "< no es una nina <EOS>\n",
      "\n",
      "> you re going to die\n",
      "= vais a morir\n",
      "< vas a morir <EOS>\n",
      "\n",
      "> you re stupid\n",
      "= eres estupido\n",
      "< eres estupido <EOS>\n",
      "\n",
      "> he is playing outdoors\n",
      "= esta jugando fuera\n",
      "< esta jugando fuera <EOS>\n",
      "\n",
      "> i m not used to working all night\n",
      "= no estoy acostumbrado a trabajar toda la noche\n",
      "< no estoy acostumbrado a trabajar toda la noche <EOS>\n",
      "\n",
      "> i m happy to be here\n",
      "= me alegro de estar aqui\n",
      "< estoy contento de estar aqui aqui <EOS>\n",
      "\n",
      "> i m sorry but right now i ve got a lot to do\n",
      "= lo siento pero ahora tengo mucho que hacer\n",
      "< creo que tengo hemorragia interna <EOS>\n",
      "\n",
      "> i m going to buy a box of matches\n",
      "= voy a comprar una caja de cerillas\n",
      "< voy a comprar una caja de cerillas <EOS>\n",
      "\n",
      "> i m still doing that\n",
      "= lo sigo haciendo\n",
      "< lo sigo haciendo eso <EOS>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "encoder.eval()\n",
    "decoder.eval()\n",
    "evaluateRandomly(encoder, decoder)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
